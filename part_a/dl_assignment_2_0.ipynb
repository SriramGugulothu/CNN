{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30673,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs23m066/Assignment_2/blob/main/dl_assignment_3_(5).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets"
      ],
      "metadata": {
        "id": "3jThqkXDJIQ0",
        "execution": {
          "iopub.status.busy": "2024-04-03T19:16:24.211839Z",
          "iopub.execute_input": "2024-04-03T19:16:24.212386Z",
          "iopub.status.idle": "2024-04-03T19:16:35.503639Z",
          "shell.execute_reply.started": "2024-04-03T19:16:24.212358Z",
          "shell.execute_reply": "2024-04-03T19:16:35.502758Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T19:16:35.505265Z",
          "iopub.execute_input": "2024-04-03T19:16:35.505937Z",
          "iopub.status.idle": "2024-04-03T19:16:35.510156Z",
          "shell.execute_reply.started": "2024-04-03T19:16:35.505886Z",
          "shell.execute_reply": "2024-04-03T19:16:35.509234Z"
        },
        "trusted": true,
        "id": "ixkPqkAFfBWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip -O nature_12K.zip\n",
        "!unzip -q nature_12K.zip"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T19:16:35.511151Z",
          "iopub.execute_input": "2024-04-03T19:16:35.511413Z",
          "iopub.status.idle": "2024-04-03T19:17:23.826124Z",
          "shell.execute_reply.started": "2024-04-03T19:16:35.511390Z",
          "shell.execute_reply": "2024-04-03T19:17:23.824802Z"
        },
        "trusted": true,
        "id": "o2UqlZXLfBWl",
        "outputId": "a37e690f-f2cf-40ca-964c-c3c94a6fc021"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "--2024-04-03 19:16:36--  https://storage.googleapis.com/wandb_datasets/nature_12K.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 142.251.180.207, 142.251.172.207, 108.177.112.207, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|142.251.180.207|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3816687935 (3.6G) [application/zip]\nSaving to: 'nature_12K.zip'\n\nnature_12K.zip      100%[===================>]   3.55G   194MB/s    in 19s     \n\n2024-04-03 19:16:55 (193 MB/s) - 'nature_12K.zip' saved [3816687935/3816687935]\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm nature_12K.zip"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T19:17:26.377205Z",
          "iopub.execute_input": "2024-04-03T19:17:26.377699Z",
          "iopub.status.idle": "2024-04-03T19:17:27.380029Z",
          "shell.execute_reply.started": "2024-04-03T19:17:26.377649Z",
          "shell.execute_reply": "2024-04-03T19:17:27.378800Z"
        },
        "trusted": true,
        "id": "g6_c5_CYfBWm",
        "outputId": "7869c52d-3ff2-4535-d453-3598815040c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "rm: cannot remove 'nature_12K.zip': No such file or directory\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "import socket\n",
        "socket.setdefaulttimeout(30)\n",
        "wandb.login()\n",
        "wandb.init(project ='DL_assignment_2')"
      ],
      "metadata": {
        "id": "NCprfCfLdHvK",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "23NOqmAzfBWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=10,numFilterss=32,sizeFilter=3,neurons=128,activFun='sigmoid',dropOut=0.0,batchNorm='no',org=0):\n",
        "        super(CNN, self).__init__()\n",
        "        self.activFunName = activFun\n",
        "        self.batchNorm = batchNorm\n",
        "        if(org ==  0):\n",
        "            numFilters = [numFilterss,numFilterss,numFilterss,numFilterss,numFilterss]\n",
        "        else:\n",
        "            numFilters = [numFilterss,numFilterss*2,numFilterss*4,numFilterss*8,numFilterss*16]\n",
        "        width = 0.0\n",
        "        hight = 0.0\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels,out_channels=numFilters[0],kernel_size=sizeFilter, stride=1)\n",
        "        width = (256 - sizeFilter)+1\n",
        "        hight = (256 - sizeFilter)+1\n",
        "        self.bn1 = nn.BatchNorm2d(numFilters[0])\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=sizeFilter, stride=2)\n",
        "        width = math.floor((width - sizeFilter)/2) + 1\n",
        "        hight = math.floor((hight -sizeFilter)/2) + 1\n",
        "\n",
        "        #print(width,hight)\n",
        "        self.conv2 = nn.Conv2d( in_channels=numFilters[0], out_channels=numFilters[1], kernel_size=sizeFilter,stride=1)\n",
        "        width = ((width - sizeFilter))+1\n",
        "        hight = ((hight-sizeFilter))+1\n",
        "        self.bn2 = nn.BatchNorm2d(numFilters[1])\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=sizeFilter,stride=2)\n",
        "        width = math.floor((width - sizeFilter)/2) + 1\n",
        "        hight = math.floor((hight -sizeFilter)/2) + 1\n",
        "\n",
        "        #print(width,hight)\n",
        "        self.conv3 = nn.Conv2d( in_channels=numFilters[1], out_channels=numFilters[2], kernel_size=sizeFilter,stride=1)\n",
        "        width = ((width - sizeFilter))+1\n",
        "        hight = ((hight-sizeFilter))+1\n",
        "        self.bn3 = nn.BatchNorm2d(numFilters[2])\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=sizeFilter,stride=2)\n",
        "        width = math.floor((width - sizeFilter)/2) + 1\n",
        "        hight = math.floor((hight -sizeFilter)/2) + 1\n",
        "\n",
        "        #print(width,hight)\n",
        "        self.conv4 = nn.Conv2d( in_channels=numFilters[2], out_channels=numFilters[3], kernel_size=sizeFilter,stride=1)\n",
        "        width = ((width - sizeFilter))+1\n",
        "        hight = ((hight-sizeFilter))+1\n",
        "        self.bn4 = nn.BatchNorm2d(numFilters[3])\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=sizeFilter,stride=1)\n",
        "        width = (width - sizeFilter)+ 1\n",
        "        hight = (hight -sizeFilter) + 1\n",
        "\n",
        "        #print(width,hight)\n",
        "        self.conv5 = nn.Conv2d( in_channels=numFilters[3], out_channels=numFilters[4], kernel_size=sizeFilter,stride=1)\n",
        "        width = ((width - sizeFilter))+1\n",
        "        hight = ((hight-sizeFilter))+1\n",
        "        self.bn5 = nn.BatchNorm2d(numFilters[4])\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=sizeFilter,stride=1)\n",
        "        width = ((width - sizeFilter)) + 1\n",
        "        hight = ((hight -sizeFilter)) + 1\n",
        "\n",
        "        #print(width,hight)\n",
        "        self.dropout = nn.Dropout(p=dropOut)\n",
        "        self.fc1 = nn.Linear(numFilters[4] * width*hight, neurons)\n",
        "        self.bn6 = nn.BatchNorm1d(neurons)\n",
        "        self.fc2 = nn.Linear(neurons,10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if(self.activFunName == 'relu'):\n",
        "            activation_fn = F.relu\n",
        "        elif(self.activFunName == 'gelu'):\n",
        "            activation_fn = F.gelu\n",
        "        elif(self.activFunName == 'silu'):\n",
        "            activation_fn = F.silu\n",
        "        else:\n",
        "            activation_fn = F.mish\n",
        "\n",
        "        if(self.batchNorm == 'yes'):\n",
        "            x = activation_fn(self.bn1(self.conv1(x)))\n",
        "        else:\n",
        "            x = activation_fn(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        if(self.batchNorm == 'yes'):\n",
        "            x = activation_fn(self.bn2(self.conv2(x)))\n",
        "        else:\n",
        "            x = activation_fn(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        if(self.batchNorm == 'yes'):\n",
        "            x = activation_fn(self.bn3(self.conv3(x)))\n",
        "        else:\n",
        "            x = activation_fn(self.conv3(x))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        if(self.batchNorm == 'yes'):\n",
        "            x = activation_fn(self.bn4(self.conv4(x)))\n",
        "        else:\n",
        "            x = activation_fn(self.conv4(x))\n",
        "        x = self.pool4(x)\n",
        "\n",
        "        if(self.batchNorm == 'yes'):\n",
        "            x = activation_fn(self.bn5(self.conv5(x)))\n",
        "        else:\n",
        "            x = activation_fn(self.conv5(x))\n",
        "        x = self.pool5(x)\n",
        "\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        if(self.batchNorm == 'yes'):\n",
        "            x = activation_fn(self.bn6(self.fc1(x)))\n",
        "        else:\n",
        "            x = activation_fn(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "FsGQWkx6JNw4",
        "execution": {
          "iopub.status.busy": "2024-04-03T19:19:55.704586Z",
          "iopub.execute_input": "2024-04-03T19:19:55.705270Z",
          "iopub.status.idle": "2024-04-03T19:19:55.730853Z",
          "shell.execute_reply.started": "2024-04-03T19:19:55.705233Z",
          "shell.execute_reply": "2024-04-03T19:19:55.729886Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "rkRs5JZogjZc",
        "execution": {
          "iopub.status.busy": "2024-04-03T19:20:02.733558Z",
          "iopub.execute_input": "2024-04-03T19:20:02.733962Z",
          "iopub.status.idle": "2024-04-03T19:20:02.802546Z",
          "shell.execute_reply.started": "2024-04-03T19:20:02.733924Z",
          "shell.execute_reply": "2024-04-03T19:20:02.801592Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,),(0.5,))])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root='inaturalist_12K/train',transform=transform)\n",
        "train_dataset,val_dataset = torch.utils.data.random_split(train_dataset,[8000,1999])\n",
        "\n",
        "transform2 = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "    transforms.RandomRotation(10),      # Randomly rotate the image by a maximum of 10 degrees\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Adjust brightness, contrast, saturation, and hue\n",
        "    transforms.RandomResizedCrop(256),  # Randomly crop and resize the image to 256x256\n",
        "    transforms.ToTensor(),              # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize((0.5,),(0.5,))  # Normalize the image\n",
        "])\n",
        "\n",
        "train_dataset2 = datasets.ImageFolder(root='inaturalist_12K/train',transform=transform2)\n",
        "train_dataset_aug,val_dataset_aug = torch.utils.data.random_split(train_dataset2,[8000,1999])\n",
        "\n",
        "def dataFun(aug,batchSize):\n",
        "    if(aug == 'no'):\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset,batch_size =batchSize,shuffle = True,num_workers=2,pin_memory=True)\n",
        "        val_loader = torch.utils.data.DataLoader(val_dataset,batch_size =batchSize,shuffle = True,num_workers=2,pin_memory=True)\n",
        "        return train_loader,val_loader\n",
        "    else:\n",
        "        train_loader_aug = torch.utils.data.DataLoader(train_dataset_aug,batch_size =batchSize,shuffle = True,num_workers=2,pin_memory=True)\n",
        "        val_loader_aug = torch.utils.data.DataLoader(val_dataset_aug,batch_size =batchSize,shuffle = True,num_workers=2,pin_memory=True)\n",
        "        return train_loader_aug,val_loader_aug"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T19:21:32.427954Z",
          "iopub.execute_input": "2024-04-03T19:21:32.428323Z",
          "iopub.status.idle": "2024-04-03T19:21:32.537095Z",
          "shell.execute_reply.started": "2024-04-03T19:21:32.428293Z",
          "shell.execute_reply": "2024-04-03T19:21:32.536121Z"
        },
        "trusted": true,
        "id": "r_POOAeIfBWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fun(neurons,numFilters,sizeFilter,activFun,optimizerName,batchSize,dropOut,num_epochs,learning_rate,batchNorm,aug,org):\n",
        "\n",
        "    train_loader,val_loader = dataFun(aug,batchSize)\n",
        "\n",
        "    in_channels = 3\n",
        "    num_classes = 10\n",
        "\n",
        "    model = CNN(in_channels, num_classes,numFilters,sizeFilter,neurons,activFun,dropOut,batchNorm,org).to(device)\n",
        "\n",
        "    if(optimizerName == 'sgd'):\n",
        "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    elif(optimizerName == 'adam'):\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    else:\n",
        "        optimizer = optim.NAdam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
        "            # Get data to cuda if possible\n",
        "            data = data.to(device=device)\n",
        "            targets = targets.to(device=device)\n",
        "            # forward\n",
        "            scores = model(data)\n",
        "            loss = criterion(scores,targets)\n",
        "            # backward\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # gradient descent or adam step\n",
        "            optimizer.step() #updates the parameters\n",
        "        train_accuracy,train_loss = check_accuracy(train_loader, model,criterion,batchSize)\n",
        "        validation_accuracy,validation_loss = check_accuracy(val_loader, model,criterion,batchSize)\n",
        "        print(f\"train_accuracy:{train_accuracy:.4f},train_loss:{train_loss:.4f}\")\n",
        "        print(f\"validation_accuracy:{validation_accuracy:.4f},validation_loss:{validation_loss:.4f}\")\n",
        "        #wandb.log({'train_accuracy':train_accuracy})\n",
        "        #wandb.log({'train_loss':train_loss})\n",
        "        #wandb.log({'val_accuracy':validation_accuracy})\n",
        "        #wandb.log({'val_loss':validation_loss})\n",
        "\n",
        "    #wandb.log({'train_accuracy':train_accuracy})\n"
      ],
      "metadata": {
        "id": "lcPp2kI_JXoJ",
        "execution": {
          "iopub.status.busy": "2024-04-03T19:20:40.219895Z",
          "iopub.execute_input": "2024-04-03T19:20:40.220273Z",
          "iopub.status.idle": "2024-04-03T19:20:40.230959Z",
          "shell.execute_reply.started": "2024-04-03T19:20:40.220245Z",
          "shell.execute_reply": "2024-04-03T19:20:40.229926Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_accuracy(loader,model,criterion,batchSize):\n",
        "    num_correct = 0\n",
        "    num_loss = 0\n",
        "    total = 0\n",
        "    num_samples = 0\n",
        "    total_loss = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "            scores = model(x)\n",
        "            loss = criterion(scores, y)\n",
        "            total_loss += loss.item()*batchSize\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum().item()\n",
        "            num_samples += predictions.size(0)\n",
        "    model.train()\n",
        "    return (num_correct / num_samples)*100 , total_loss"
      ],
      "metadata": {
        "id": "ZYuVL_f2RKY7",
        "execution": {
          "iopub.status.busy": "2024-04-03T19:20:58.691758Z",
          "iopub.execute_input": "2024-04-03T19:20:58.692715Z",
          "iopub.status.idle": "2024-04-03T19:20:58.701997Z",
          "shell.execute_reply.started": "2024-04-03T19:20:58.692673Z",
          "shell.execute_reply": "2024-04-03T19:20:58.700832Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neurons = 128\n",
        "numFilters = 32\n",
        "sizeFilter = 3\n",
        "activFun = 'relu'\n",
        "optimizer = 'adam'\n",
        "batchSize=32\n",
        "dropOut = 0\n",
        "num_epochs = 5\n",
        "learning_rate = 1e-3\n",
        "batchNorm = 'yes'\n",
        "aug='yes'\n",
        "org = 1\n",
        "train_fun(neurons,numFilters,sizeFilter,activFun,optimizer,batchSize,dropOut,num_epochs,learning_rate,batchNorm,aug,org)"
      ],
      "metadata": {
        "id": "z6pHMzFjSJWt",
        "execution": {
          "iopub.status.busy": "2024-04-03T19:31:42.926532Z",
          "iopub.execute_input": "2024-04-03T19:31:42.927362Z",
          "iopub.status.idle": "2024-04-03T19:41:23.878098Z",
          "shell.execute_reply.started": "2024-04-03T19:31:42.927321Z",
          "shell.execute_reply": "2024-04-03T19:41:23.876601Z"
        },
        "trusted": true,
        "outputId": "49b86703-f7c6-442d-8b90-e77f0f9a3c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 250/250 [04:08<00:00,  1.01it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "train_accuracy:22.7000,train_loss:17209.0798\nvalidation_accuracy:22.2611,validation_loss:4341.5697\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "  8%|▊         | 20/250 [00:20<03:58,  1.04s/it]\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m aug\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m org \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneurons\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnumFilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43msizeFilter\u001b[49m\u001b[43m,\u001b[49m\u001b[43mactivFun\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdropOut\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatchNorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43maug\u001b[49m\u001b[43m,\u001b[49m\u001b[43morg\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[14], line 20\u001b[0m, in \u001b[0;36mtrain_fun\u001b[0;34m(neurons, numFilters, sizeFilter, activFun, optimizerName, batchSize, dropOut, num_epochs, learning_rate, batchNorm, aug, org)\u001b[0m\n\u001b[1;32m     17\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader)):\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# Get data to cuda if possible\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     23\u001b[0m         targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QNvLt5vXfBWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_fun():\n",
        "    wandb.init(project ='DL_assignment_2')\n",
        "    params = wandb.config\n",
        "    with wandb.init(project = 'DL_assignment_2', name='neurons'+str(params.neurons)+'filterSize'+str(params.sizeFilter)+'activFun'+params.activFun) as run:\n",
        "        train_fun(params.neurons,params.sizeFilter,params.activFun,params.optimizer,params.batchSize,params.dropOut,params.num_epochs,params.learning_rate)\n",
        "\n",
        "sweep_params = {\n",
        "    'method' : 'bayes',\n",
        "    'name'   : 'Shriram',\n",
        "    'metric' : {\n",
        "        'goal' : 'maximize',\n",
        "        'name' : 'val_accuracy',\n",
        "    },\n",
        "    'parameters' : {\n",
        "        'neurons':{'values':[128,256]},\n",
        "        'sizeFilter':{'values' : [3,5]},\n",
        "        'activFun' :{'values':['relu','gelu','silu','mish']},\n",
        "        'optimizer' :{'values':['adam','nadam']},\n",
        "        'batchSize' : {'values':[32,64]},\n",
        "        'dropOut' :{'values':[0.1]},\n",
        "        'num_epochs':{'values':[5,10]},\n",
        "        'learning_rate' :{'values':[1e-3,1e-4]}\n",
        "    }\n",
        "}\n",
        "sweepId = wandb.sweep(sweep_params,project = 'DL_assignment_2')\n",
        "wandb.agent(sweepId,function =main_fun,count = 10)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "YzBw7Y4RDjyp",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
